<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">How to handle Exceptions in JAX-RS applications</title><link rel="alternate" href="http://www.mastertheboss.com/uncategorised/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/uncategorised/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications</id><updated>2021-12-21T10:02:32Z</updated><content type="html">This article will teach you how to handle Exceptions properly in RESTful Web services using JAX-RS API and some advanced options which are available with RESTEasy and Quarkus runtime. Overview of REST Exceptions As for every Java classes, REST Endpoints can throw in their code both checked Exceptions (i.e. classes extending java.lang.Exception) and unchecked (i.e. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Prevent Python dependency confusion attacks with Thoth</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" /><author><name>Fridolin Pokorny</name></author><id>ae654f04-c49e-4f00-acd0-060b226f40bf</id><updated>2021-12-21T07:00:00Z</updated><published>2021-12-21T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; became popular as a casual scripting language but has since evolved into the corporate space, where it is used for &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; applications, among others. Because Python is a high-level programming language, developers often use it to quickly prototype applications. &lt;a href="https://docs.python.org/3/extending/extending.html"&gt;Python native extensions&lt;/a&gt; make it easy to optimize any computation-intensive parts of the application using a lower-level programming language like &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For applications that need to scale, we can use &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python Source-to-Image tooling&lt;/a&gt; (S2I) to convert a Python application into a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image. That image can then be orchestrated and scaled using cluster orchestrators such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. All of these features together provide a convenient platform for solving problems using Python-based solutions that scale, are maintainable, and are easily extensible.&lt;/p&gt; &lt;p&gt;As a community-based project, the main source of open-source Python packages is the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; (PyPI). As of this writing, PyPI hosts more than 3 million releases, and the number of releases available continues to grow exponentially. PyPI's growth is an indicator of Python's popularity worldwide.&lt;/p&gt; &lt;p&gt;However, Python's community-driven dependency resolvers were not designed for corporate environments, and that has led to dependency management issues and vulnerabilities in the Python ecosystem. This article describes some of the risks involved in resolving Python dependencies and introduces &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;'s tools for avoiding them.&lt;/p&gt; &lt;h2&gt;Dependency management in Python&lt;/h2&gt; &lt;p&gt;The Python package installer, &lt;a href="https://pypi.org/project/pip"&gt;pip&lt;/a&gt;, is a popular tool for resolving Python application dependencies. Unfortunately, pip does not provide a way to manage lock files for application dependencies. Pip resolves dependencies to the latest possible versions at the given point in time, so the resolution is highly dependent on the time when the resolution process was triggered. Dependency problems such as &lt;em&gt;overpinning&lt;/em&gt; (requesting too wide a range of versions) frequently introduce issues to the Python application stack.&lt;/p&gt; &lt;p&gt;To address lock file management issues, the Python community developed tools such as &lt;a href="https://pypi.org/project/pip-tools/"&gt;pip-tools&lt;/a&gt;, &lt;a href="https://pipenv.pypa.io/"&gt;Pipenv&lt;/a&gt;, and &lt;a href="https://python-poetry.org/"&gt;Poetry&lt;/a&gt;. (Our &lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications"&gt;article introducing micropipenv&lt;/a&gt; includes an overview of these projects.)&lt;/p&gt; &lt;p&gt;The &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; is the primary index consulted by pip. In some cases, applications need libraries from other Python package indexes. For these, pip provides the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-index-url"&gt;--index-url&lt;/a&gt; and &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-extra-index-url"&gt;--extra-index-url&lt;/a&gt; options. Most of the time, there are two primary reasons you might need to install dependencies from Python package sources other than PyPI:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Installing specific builds of packages whose features cannot be expressed using wheel tags, or that do not meet &lt;a href="https://github.com/pypa/manylinux"&gt;manylinux standards&lt;/a&gt;; e.g., the &lt;a href="https://tensorflow.pypi.thoth-station.ninja/"&gt;AVX2-enabled builds of TensorFlow&lt;/a&gt; hosted on the Python package index of the Artificial Intelligence Center of Excellence (AICoE).&lt;/li&gt; &lt;li&gt;Installing packages that should not be hosted on PyPI, such as packages specific to one company or patched versions of libraries used only for testing.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why Python is vulnerable to dependency confusion attacks&lt;/h2&gt; &lt;p&gt;The pip options &lt;code&gt;--index-url&lt;/code&gt; and &lt;code&gt;--extra-index-url&lt;/code&gt; provide a way to specify alternate Python package indexes for resolving and installing Python packages. The first option, &lt;code&gt;--index-url&lt;/code&gt;, specifies the main Python package index for resolving Python packages, and defaults to PyPI. When you need a second package index, you can include the &lt;code&gt;--extra-index-url&lt;/code&gt; option as many times as needed. The resolution logic in pip first uses the main index, then, if the required package or version is not found there, it checks the secondary indexes.&lt;/p&gt; &lt;p&gt;Thus, although you can specify the order in which indexes are consulted, the configuration is not specified for each package individually. Moreover, the index configuration is applied for transitive dependencies introduced by direct dependencies, as well.&lt;/p&gt; &lt;p&gt;To bypass this order, application developers can manage requirements with hashes that are checked during installation and resolution to differentiate releases. This solution is unintuitive and error-prone, however. Although we encourage keeping hashes in lock files for integrity checks, they should be managed automatically using the appropriate tools.&lt;/p&gt; &lt;p&gt;Now, let’s imagine a dependency named &lt;code&gt;foo&lt;/code&gt; that a company uses on a private package index. Suppose a different package with the same name is hosted on PyPI. An unexpected glitch—such as a temporary network issue when resolving the company private package index—could lead the application to import the &lt;code&gt;foo&lt;/code&gt; package from PyPI in default setups. In the worst case, the package published on PyPI might be a &lt;a href="https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610"&gt;malicious alternative&lt;/a&gt; that reveals company secrets to an attacker.&lt;/p&gt; &lt;p&gt;This issue also applies to pip-tools, Pipenv, and Poetry). Pipenv provides a way to configure a Python package index for a specific package, but it does not enforce the specified configuration. All the mentioned dependency resolution tools treat multiple Python package indexes supplied as mirrors.&lt;/p&gt; &lt;h2&gt;Using Thoth to resolve dependency confusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://thoth-station.ninja/"&gt;Thoth&lt;/a&gt; is a project sponsored by Red Hat that takes a fresh look at the complex needs of Python applications and &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;moves the resolution process to the cloud&lt;/a&gt;. Naturally, being cloud-based has its advantages and disadvantages depending on how the tool is used.&lt;/p&gt; &lt;p&gt;Because Thoth moves dependency resolution to the cloud, a central authority can resolve application requirements. This central authority can be configured with fine-grained control over which application dependencies go into desired environments. For instance, you could handle dependencies in test environments and production environments differently.&lt;/p&gt; &lt;p&gt;Thoth's resolver pre-aggregates information about Python packages from various Python package indexes. This way, the resolver can monitor Python packages published on PyPI, on the AICoE-specific TensorFlow index, on a &lt;a href="https://www.operate-first.cloud/community-handbook/pulp/usage.md"&gt;corporate Pulp Python index&lt;/a&gt;, on the &lt;a href="https://download.pytorch.org/whl/cu111/"&gt;PyTorch CUDA 11.1 index&lt;/a&gt;, and on &lt;a href="https://download.pytorch.org/whl/cpu/"&gt;builds for CPU use&lt;/a&gt;, which the PyTorch community provides for specific cases. Moreover, the cloud-based resolver &lt;a href="https://thoth-station.ninja/docs/developers/adviser/security.html"&gt;introspects the published packages with respect to security&lt;/a&gt; or &lt;a href="https://github.com/thoth-station/cve-update-job"&gt;vulnerabilities&lt;/a&gt; (see &lt;a href="https://github.com/pypa/advisory-db"&gt;PyPA’s Python Packaging Advisory Database&lt;/a&gt;) to additionally guide a &lt;a href="https://developers.redhat.com/articles/2021/09/29/secure-your-python-applications-thoth-recommendations"&gt;secure resolution process&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;contact the Thoth team&lt;/a&gt; if you wish to register your own Python package index to Thoth.&lt;/p&gt; &lt;h3&gt;Solver rules in Thoth&lt;/h3&gt; &lt;p&gt;A central authority can be configured to allow or block packages or specific package releases that are hosted on the Python package indexes. This feature is called &lt;em&gt;solver rules&lt;/em&gt; and is maintained by a Thoth operator.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://thoth-station.ninja/docs/developers/adviser/deployment.html#configuring-solver-rules"&gt;Configuring solver rules&lt;/a&gt; in the Thoth documentation for more about this topic. Also check out our &lt;a href="https://www.youtube.com/watch?v=wjMNOyGupbs"&gt;YouTube video demonstrating solver rules&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use solver rules to allow the Thoth operator to specify which Python packages or specific releases can be considered during the resolution process, respecting the Python package indexes registered when a request is made to the cloud-based resolver. You can also use solver rules to block the analysis of packages that are considered too old, are no longer supported, or simply don't adhere to company policies.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;Report issues with open source Python packages&lt;/a&gt; to help us create new solver rules.&lt;/p&gt; &lt;h3&gt;Strict index configuration&lt;/h3&gt; &lt;p&gt;Another feature in Thoth is the ability to &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;configure a strict Python package index configuration&lt;/a&gt;. By default, the recommendation engine considers all the packages published on the indexes it monitors and uses a &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; to come up with a set of packages that are considered most appropriate. However, in some situations, Thoth users want to suppress this behavior and explicitly configure Python package indexes for consuming Python packages on their own.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are interested in the strict index configuration, please &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;browse the documentation&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=p6fjVQ0aUPE"&gt;watch our video demonstration&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prescriptions&lt;/h3&gt; &lt;p&gt;Thoth also supports a &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;mechanism called prescriptions&lt;/a&gt; that provides additional, detailed guidelines for package resolution. Prescriptions are analogous to manifests in Kubernetes and OpenShift. A manifest lists the desired state of the cluster, and the machinery behind the cluster orchestrator tries to create and maintain the desired state. Similarly, prescriptions provide a declarative way to specify the resolution process for the particular dependencies and Python package indexes used.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See the &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;prescriptions section&lt;/a&gt; in the Thoth documentation for more about this feature. You can also browse Thoth's &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt; for prescriptions available for open source Python projects. See our &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;article about prescriptions&lt;/a&gt; for more insight into this concept.&lt;/p&gt; &lt;p&gt;Thoth's &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; searches for a solution that satisfies application requirements, taking prescriptions into account. This algorithm provides the power to adjust the resolution process in whatever manner users desire. Adjustments to the resolution process can be made using &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription/should_include.html#should-include-labels"&gt;labeled requests to the resolver&lt;/a&gt; which can pick prescriptions that match specified criteria written in YAML files. An example can be consuming all the packages solely from one package index (such as a Python package index hosted using &lt;a href="https://pulpproject.org/"&gt;Pulp&lt;/a&gt;) that hosts packages that can be considered as trusted for Thoth users.&lt;/p&gt; &lt;h2&gt;About Project Thoth&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow project updates, please &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;subscribe to our YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation Twitter handle&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" title="Prevent Python dependency confusion attacks with Thoth"&gt;Prevent Python dependency confusion attacks with Thoth&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2021-12-21T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Architectural introduction</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-architetural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-architetural-introduction.html</id><updated>2021-12-21T06:00:00Z</updated><content type="html">Part 1 - Architectural introduction The last few months we have been digging deeply into the world of healthcare architectures with a focus on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in that we have the mission of creating architectural content based on common customer adoption patterns.  That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Portfolio Architecture.  Let's look at these architectures, how they're created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division into smaller parts. In this case presented here is we are looking closer at the healthcare industry and an intelligent data as a service (iDaaS) architecture. This use case we've defined as the following: iDaaS is all about transforming the way the healthcare industry interacts with data and information. It provides an example for connecting, processing and leveraging clinical, financial, administrative, and life sciences data at scale in a consistent manner.  The approach taken is to research our existing customers that have implemented solutions in this space, collect their public-facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architecture for use in telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on intelligent data as a service architecture: 1. 2. Common architectural elements 3. Example iDaaS architecture 4. Example HL7 and FHIR integration architecture 5. Example iDaaS knowledge and insight architecture Catch up on any past articles you missed by following any published links above. Next in this series, we will take a look at the generic common architectural elements for the intelligent data as a service architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">How to Integrate Keycloak for Authentication with Apache APISIX</title><link rel="alternate" href="https://www.keycloak.org/2021/12/apisix" /><author><name>Xinxin Zhu &amp; Yilin Zeng</name></author><id>https://www.keycloak.org/2021/12/apisix</id><updated>2021-12-21T00:00:00Z</updated><content type="html">This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in Apache APISIX through detailed steps. is an open source identity and access management solution for modern applications and services. Keycloak supports Single-Sign On, which enables services to interface with Keycloak through protocols such as OpenID Connect, OAuth 2.0, etc. Keycloak also supports integrations with different authentication services, such as Github, Google and Facebook. In addition, Keycloak also supports user federation, and can import users through LDAP and Kerberos. For more information about Keycloak, please refer to the . is a dynamic, real-time, high-performance API gateway, providing rich traffic management. The project offers load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and many useful plugins. In addition, the gateway supports dynamic plugin changes along with hot update. The OpenID Connect plugin for Apache APISIX allows users to replace traditional authentication mode with centralized identity authentication mode via OpenID Connect. HOW TO USE INSTALL APACHE APISIX INSTALL DEPENDENCIES The Apache APISIX runtime environment requires dependencies on NGINX and etcd. Before installing Apache APISIX, please install dependencies according to the operating system you are using. We provide the dependencies installation instructions for CentOS7, Fedora 31 and 32, Ubuntu 16.04 and 18.04, Debian 9 and 10, and macOS. Please refer to for more details. INSTALLATION VIA RPM PACKAGE (CENTOS 7) This installation method is suitable for CentOS 7; please run the following command to install Apache APISIX. sudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm INSTALLATION VIA DOCKER Please refer to . INSTALLATION VIA HELM CHART Please refer to . INITIALIZING DEPENDENCIES Run the following command to initialize the NGINX configuration file and etcd. make init START APACHE APISIX Run the following command to start Apache APISIX. apisix start START KEYCLOAK Here we use docker to start Keycloak. docker run -p 8080:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=password -e DB_VENDOR=h2 -d jboss/keycloak:9.0.2 After execution, you need to verify that Keycloak and postgres have started successfully. docker-compose ps CONFIGURE KEYCLOAK After Keycloak is started, use your browser to access "http://127.0.0.1:8080/auth/admin/" and type the admin/password account password to log in to the administrator console. CREATE A REALM First, you need to create a realm named apisix_test_realm. In Keycloak, a realm is a workspace dedicated to managing projects, and the resources of different realms are isolated from each other. The realm in Keycloak is divided into two categories: one is the master realm, which is created when Keycloak is first started and used to manage the admin account and create other realm. the second is the other realm, which is created by the admin in the master realm and can be used to create, manage and use users and applications in this realm. The second category is the other realm, created by admin in the master realm, where users and applications can be created, managed and used. For more details, please refer to the . CREATE A CLIENT The next step is to create the OpenID Connect Client. In Keycloak, Client means a client that is allowed to initiate authentication to Keycloak. In this example scenario, Apache APISIX is equivalent to a client that is responsible for initiating authentication requests to Keycloak, so we create a Client with the name apisix. More details about the Client can be found in . CONFIGURE THE CLIENT After the Client is created, you need to configure the Apache APISIX access type for the Client. In Keycloak, there are three types of Access Type: 1. Confidential: which is used for applications that need to perform browser login, and the client will get the access token through client secret, mostly used in web systems rendered by the server. 2. Public: for applications that need to perform browser login, mostly used in front-end projects implemented using vue and react. 3. Bearer-only: for applications that don’t need to perform browser login, only allow access with bearer token, mostly used in RESTful API scenarios. For more details about Client settings, please refer to . Since we are using Apache APISIX as the Client on the server side, we can choose either "confidential" Access Type or "Bearer-only" Access Type. For the demonstration below, we are using "confidential" Access Type as an example. CREATE USERS Keycloak supports interfacing with other third-party user systems, such as Google and Facebook, or importing or manually creating users using LDAP . Here we will use "manually creating users" to demonstrate. alt=Set Client type,width=640,height=350 Then set the user’s password in the Credentials page. CREATE ROUTES After Keycloak is configured, you need to create a route and open the Openid-Connect plugin . For details on the configuration of this plugin, please refer to the . GET CLIENT_ID AND CLIENT_SECRET In the above configuration. * client_id is the name used when creating the Client before, i.e. apisix * client_secret should be obtained from Clients-apisix-Credentials, for example: d5c42c50-3e71-4bbbe-aa9e-31083ab29da4. GET THE DISCOVERY CONFIGURATION Go to Realm Settings-General-Endpoints, select the OpenID Endpoint Configuration link and copy the address that the link points to, for example:`http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration`. CREATE A ROUTE AND ENABLE THE PLUG-IN Use the following command to access the Apache APISIX Admin interface to create a route, set the upstream to httpbin.org, and enable the plug-in OpenID Connect for authentication. Note: If you select bearer-only as the Access Type when creating a Client, you need to set bearer_only to true when configuring the route, so that access to Apache APISIX will not jump to the Keycloak login screen. curl -XPOST 127.0.0.1:9080/apisix/admin/routes -H "X-Api-Key: edd1c9f034335f136f87ad84b625c8f1" -d '{ "uri":"/*", "plugins":{ "openid-connect":{ "client_id":"apisix", "client_secret":"d5c42c50-3e71-4bbe-aa9e-31083ab29da4", "discovery":"http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration", "scope":"openid profile", "bearer_only":false, "realm":"apisix_test_realm", "introspection_endpoint_auth_method":"client_secret_post", "redirect_uri":"http://127.0.0.1:9080/" } }, "upstream":{ "type":"roundrobin", "nodes":{ "httpbin.org:80":1 } } }' ACCESS TESTING Once the above configuration is complete, we are ready to perform the relevant access tests in Apache APISIX. ACCESS APACHE APISIX Use your browser to access . Since the OpenID-Connect plugin is enabled and bearer-only is set to false, when you access this path for the first time, Apache APISIX will redirect to the login screen configured in apisix_test_realm in Keycloak and make a user login request. Enter the User peter created during the Keycloak configuration to complete user login. SUCCESSFUL ACCESS After a successful login, the browser will again redirect the link to and will successfully access the image content. The content is identical to that of the upstream . LOGOUT After the test, use your browser to access http:/127.0.0.1:9080/logout to logout your account. Note: The logout path can be specified by logout_path in the OpenID-Connect plug-in configuration, the default is logout. SUMMARY This article shows the procedure of using OpenID-Connect protocol and Keycloak for authentication in Apache APISIX. By integrating with Keycloak, Apache APISIX can be configured to authenticate and authenticate users and application services, which greatly reduces the development work involved. For more information about the implementation of authentication in Apache APISIX, see .</content><dc:creator>Xinxin Zhu &amp; Yilin Zeng</dc:creator></entry><entry><title>Prevent auto-reboot during Argo CD sync with machine configs</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/20/prevent-auto-reboot-during-argo-cd-sync-machine-configs" /><author><name>Ishita Sequeira</name></author><id>c683a603-6c02-4203-bafe-2fa97fb1e808</id><updated>2021-12-20T07:00:00Z</updated><published>2021-12-20T07:00:00Z</published><summary type="html">&lt;p&gt;Nodes in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; can be updated automatically through OpenShift's Machine Config Operator (MCO). A machine config is a custom resource that helps a cluster manage the complete life cycle of its nodes. When a machine config resource is created or updated in a cluster, the MCO picks up the update, performs the necessary changes to the selected nodes, and restarts the nodes gracefully by cordoning, draining, and rebooting them. The MCO handles everything ranging from the kernel to the kubelet.&lt;/p&gt; &lt;p&gt;However, interactions between the MCO and the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps workflow&lt;/a&gt; can introduce major performance issues and other undesired behavior. This article shows how to make the MCO and the &lt;a href="https://argoproj.github.io"&gt;Argo CD&lt;/a&gt; GitOps orchestration tool work well together.&lt;/p&gt; &lt;h2&gt;Machine configs and Argo CD: Performance challenges&lt;/h2&gt; &lt;p&gt;When using machine configs as part of a GitOps workflow, the following sequence can produce suboptimal performance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Argo CD starts a &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/"&gt;sync job&lt;/a&gt; after a commit to the Git repository containing application resources.&lt;/li&gt; &lt;li&gt;If Argo CD notices a new or changed machine config while the sync operation is ongoing, MCO picks up the change to the machine config and starts rebooting the nodes to apply it.&lt;/li&gt; &lt;li&gt;If any of the nodes that are rebooting contain the Argo CD application controller, the application controller terminates and the application sync is aborted.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Because the MCO reboots the nodes in sequential order, and the Argo CD workloads can be rescheduled on each reboot, it could take some time for the sync to be completed. This could also result in undefined behavior until the MCO has rebooted all nodes affected by the machine configs within the sync.&lt;/p&gt; &lt;h2&gt;Extend the application's manifest in Git&lt;/h2&gt; &lt;p&gt;The solution to the interactions in the previous section requires you to extend the application's manifest in Git by adding PreSync and PostSync hooks to Argo CD. Argo CD provides these hooks so that you can ensure that operations of your choice are performed before and after each sync (Figure 1). As the name suggests, a PreSync hook is a job that Argo CD executes right before the sync starts. Similarly, the PostSync hook executes after a sync.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Sync Hook Workflow" data-entity-type="file" data-entity-uuid="9b3885cd-4797-49b5-9175-e43d73cedca5" height="317" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-11-24%20at%2012.06.09%20AM.png" width="943" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. Sync hook workflow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We will use &lt;a href="https://github.com/ishitasequeira/kam-blog.git"&gt;kam-blog&lt;/a&gt; as the sample application for this demo. We have generated this application following directions in the article &lt;a href="https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli"&gt;Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Add sync hooks to Argo CD&lt;/h3&gt; &lt;p&gt;Our PreSync job pauses the Machine Config Pool (MCP) so it does not reboot the nodes in order to apply the machine config changes. We ensure this pause by setting the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To insert the PreSync job, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/pre-sync-job.yaml"&gt;pre-sync-job.yaml&lt;/a&gt; and add it to the same directory as the application. The content of the file is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: batch/v1 kind: Job metadata: annotations: argocd.argoproj.io/hook: PreSync argocd.argoproj.io/hook-delete-policy: HookSucceeded name: mcp-worker-pause-job namespace: openshift-gitops spec: template: spec: containers: - image: registry.redhat.io/openshift4/ose-cli:v4.4 command: - /bin/bash - -c - | echo -n "Waiting for the MCP $MCP to converge." echo $(oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/$MCP) sleep $SLEEP echo "DONE" imagePullPolicy: IfNotPresent name: mcp-worker-pause-job env: - name: SLEEP value: "10" - name: MCP value: worker restartPolicy: Never serviceAccount: sync-job-sa &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The PostSync hook resumes the MCP so that it reboots the nodes, applying the queued or incoming machine config changes. Enable this behavior by setting the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;. To insert the PostSync job, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/post-sync-job.yaml"&gt;post-sync-job.yaml&lt;/a&gt; and add it to the same directory as the application. The content of the file is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: batch/v1 kind: Job metadata: annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded name: mcp-worker-resume-job namespace: openshift-gitops spec: template: spec: containers: - image: registry.redhat.io/openshift4/ose-cli:v4.4 command: - /bin/bash - -c - | echo -n "Waiting for the MCP $MCP to converge." sleep $SLEEP echo $(oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/$MCP) echo "DONE" imagePullPolicy: Always name: mcp-worker-resume-job env: - name: SLEEP value: "5" - name: MCP value: worker dnsPolicy: ClusterFirst restartPolicy: OnFailure serviceAccount: sync-job-sa serviceAccountName: sync-job-sa terminationGracePeriodSeconds: 30&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Add permissions for Sync Hooks&lt;/h3&gt; &lt;p&gt;In order for these jobs to execute successfully, they need permissions to manipulate machine config resources in the cluster. These permissions need to be granted using a &lt;code&gt;ServiceAccount&lt;/code&gt; and appropriate &lt;code&gt;ClusterRole&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; properties.&lt;/p&gt; &lt;p&gt;To add the &lt;code&gt;ServiceAccount&lt;/code&gt;, &lt;code&gt;ClusterRole&lt;/code&gt;, and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; properties, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/sync-job-cluster-rbac.yaml"&gt;sync-job-cluster-rbac.yaml&lt;/a&gt; and add it to the same directory as the application. The content is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ServiceAccount metadata: annotations: {} name: sync-job-sa namespace: openshift-gitops --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: sync-job-sa-role rules: - apiGroups: - apiextensions.k8s.io - machineconfiguration.openshift.io resources: - machineconfigpools verbs: - get - list - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: sync-job-sa-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: sync-job-sa-role subjects: - kind: ServiceAccount name: sync-job-sa namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now apply the configuration to the cluster using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -k config/argocd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you have applied the configuration, try manually syncing the application. You should see that the PreSync and PostSync jobs have paused and unpaused the MCP as shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/hooks.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/hooks.png?itok=4e1Mb3AJ" width="1440" height="398" alt="The OpenShift user interface shows the actions of the PreSync and PostSync hooks." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The OpenShift user interface shows the actions of the PreSync and PostSync hooks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You can also see that the MCP paused by examining its details (Figure 3).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/paused.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/paused.png?itok=_Py2jKAE" width="1440" height="870" alt="Machine Config Pool details show that it is paused." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Machine Config Pool details show that it is paused. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the sync job finishes, the PostSync job unpauses the MCP and resumes all the updates to the nodes in the cluster. The MCP details show this change as well (Figure 4).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unpaused.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/unpaused.png?itok=IcMVT1Z2" width="1440" height="890" alt="Machine Config Pool details show that it is unpaused." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Machine Config Pool details show that it is unpaused. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If the sync fails for any reason, the MCP will stay paused and won't update the nodes. To resume MCP updates, you have to manually update the MCP and set the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;. You can set the flag using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/worker&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Updates to machine configs can lead to uncontrolled node reboots, termination of the sync process, and unanticipated issues in the application. The workaround in this article helps to prevent nodes from rebooting while the critical Argo CD sync operations are in progress.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/20/prevent-auto-reboot-during-argo-cd-sync-machine-configs" title="Prevent auto-reboot during Argo CD sync with machine configs"&gt;Prevent auto-reboot during Argo CD sync with machine configs&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ishita Sequeira</dc:creator><dc:date>2021-12-20T07:00:00Z</dc:date></entry><entry><title type="html">How to fix Log4j CVE-2021-44228</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-handle-cve-2021-44228-in-java-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-cve-2021-44228-in-java-applications" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-handle-cve-2021-44228-in-java-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-cve-2021-44228-in-java-applications</id><updated>2021-12-20T05:14:00Z</updated><content type="html">This security bulletin discusses the recent flaws in Log4j2 library which affects some recent versions of this opensource library. Let’s see how to perform checks and mitigation actions if your Java applications are using a flawed Log4j2 version. At the time of writing (20 December 2021), Log4j has the following vulnerabilities: CVE-2021-44228 (CVSS:10) CVE-2021-45046 (CVSS:10) ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">This Week in JBoss - 20 December 2021</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2021-12-20.html" /><category term="cve" /><category term="log4j" /><category term="quarkus" /><category term="resteasy" /><category term="elytron" /><category term="keycloak" /><category term="wildfly" /><author><name>Romain Pelisse</name><uri>https://www.jboss.org/people/romain-pelisse</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-12-20.html</id><updated>2021-12-20T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="cve,log4j,quarkus,resteasy,elytron,keycloak,wildfly"&gt; &lt;h1&gt;This Week in JBoss - 20 December 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;&lt;em&gt;Hello! With a bit of delay, here is our very last editorial of the year! Sadly, it focus point has to be the log4j vulnerability,but we also ensured there was some interesting and exciting news too. Enjoy and happy holidays!&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_log4j_security_vulnerabilities"&gt;log4j Security vulnerabilities&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Sadly, we have to start our editorial with a less-than-ideal piece of news. In case, you somehow missed, be aware that some vulnerabilities have been found in Log4J. Being one of the most used Java library, it is a rather critical issue. Please take look at this article to know &lt;a href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-handle-cve-2021-44228-in-java-applications/?utm_source=rss&amp;#38;utm_medium=rss&amp;#38;utm_campaign=how-to-handle-cve-2021-44228-in-java-applications"&gt;How to fix Log4j CVE-2021-44228&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Some other projects from the JBoss community may also have been impacted. Please check any project you are currently using and see if, like &lt;a href="https://blog.kie.org/2021/12/kie-log4j2-exploit-cve-2021-44228.html"&gt;KIE&lt;/a&gt; or &lt;a href="https://infinispan.org/blog/2021/12/13/infinispan-log4j-cve-releases"&gt;InfiniSpan&lt;/a&gt;, they have provided information on the impact of the vulnerability.&lt;/p&gt; &lt;p&gt;Note that, on the bright side, &lt;a href="https://quarkus.io/blog/quarkus-and-CVE-2021-4428/"&gt;Quarkus is not affected by the Log4J Vulnerability&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_wildfly_26_is_here"&gt;Wildfly 26 is here!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/12/16/WildFly26-Final-Released/"&gt;Wildfly 26 has been released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/12/16/WildFly-s2i-26-Released/"&gt;WildFly 26 S2I images have been released on quay.io&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_security_concern"&gt;Security concern&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;With the nasty security vulnereablity on log4j, it’s certainly time to look at some new security features coming in either Elytron or Wildly (or both):&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/new-security-features-wildfly/"&gt;New Security Features in WildFly&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/jaas-realm/"&gt;Using a JAAS realm in Elytron&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/upcoming-filesystem-encryption-integrity/"&gt;Upcoming filesystem realm encryption and integrity support in Elytron&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/client-side-jvm-wide-default-sslcontext/"&gt;Upcoming client side default SSL context provider&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://wildfly-security.github.io/wildfly-elytron/blog/securing-wildfly-apps-openid-connect/"&gt;Securing WildFly Apps with OpenID Connect&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_quarkus"&gt;Quarkus&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Always on top of the latest new feature, Quarkus offers you to &lt;a href="https://developers.redhat.com/articles/2021/12/14/explore-java-17-language-features-quarkus"&gt;Explore Java 17 language features with Quarkus&lt;/a&gt;! If you already hook to Quarkus and love how it allows you to deploy native executables, you’ll probably be interested by this article on &lt;a href="https://quarkus.io/blog/upx/"&gt;Compressing native executables with UPX&lt;/a&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases…​&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;As always, the JBoss community has been quite actived and released quite a lot:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/12/keycloak-1511-released"&gt;Keycloak 15.1.1 released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/12/keycloak-1600-released"&gt;Keycloak 16.0.0 released&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-5-3-final-released/"&gt;Quarkus 2.5.3.Final released - Maintenance release&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-2-2-1_and_4-1-7-1/"&gt;Eclipse Vert.x Webauthn 4.2.2.1 and 4.1.7.1 released!&lt;/a&gt; along with some previous releases:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-3-9-11/"&gt;Eclipse Vert.x 3.9.11 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-7/"&gt;Eclipse Vert.x 4.1.7 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-2-2/"&gt;Eclipse Vert.x 4.2.2 released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org//news/2021/12/16/WildFly26-Final-Released/"&gt;Wildfly 26 is released!&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again next year for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/romain-pelisse.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Romain Pelisse&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Romain Pelisse</dc:creator></entry><entry><title>How to connect Prometheus to OpenShift Streams for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" /><author><name>Pete Muir</name></author><id>8568af74-782f-4e57-8d7b-519de49b8526</id><updated>2021-12-17T07:00:00Z</updated><published>2021-12-17T07:00:00Z</published><summary type="html">&lt;p&gt;You've always been able to view some metrics for &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; in the UI or access them via the API. We recently added a feature to OpenShift Streams for Apache Kafka that exports these metrics to &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt;, a system monitoring and alerting toolkit. Connecting these services lets you view your exported metrics alongside your other Kafka cluster metrics.&lt;/p&gt; &lt;p&gt;This article shows you how to set up OpenShift Streams for Apache Kafka to export metrics to Prometheus. The example configuration includes integration with &lt;a href="https://grafana.com"&gt;Grafana&lt;/a&gt;, a data visualization application that is often used with Prometheus. Almost all observability products and services support Prometheus, so you can easily adapt what you learn in this article to your observability stack.&lt;/p&gt; &lt;h2&gt;Guides and prerequisites&lt;/h2&gt; &lt;p&gt;We will use a number of guides for this configuration; You can open them now or use the links in each section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated: Creating a cluster&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;Configuring a GitHub identity provider&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;OpenShift Dedicated: Managing administration roles and users&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will also use these examples from the Prometheus and Grafana projects, respectively:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;Additional scrape configs for Prometheus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Set up your Kubernetes cluster&lt;/h2&gt; &lt;p&gt;To begin, you need to set up a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster to run Prometheus and Grafana. The example in this article will use &lt;a href="https://cloud.redhat.com/products/dedicated/?intcmp=701f20000012jmYAAQ"&gt;Red Hat OpenShift Dedicated&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Start by following the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated guide&lt;/a&gt; to creating a Customer Cloud Subscription cluster on Amazon Web Services. Then, follow the instruction to &lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;configure a GitHub identity provider&lt;/a&gt; so that you can use your GitHub ID to log in to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. (Other options are available, but we're using these methods for the example.) Once you've got things configured, your OpenShift Cluster Manager should look like the screenshot in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig1.png?itok=B6P_G0hA" width="600" height="147" alt="The OpenShift Cluster Manager with GitHub configured as an identity provider." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Configure GitHub as an identity provider for logging in to the OpenShift Dedicated console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Finally, give your GitHub user the &lt;code&gt;cluster-admin&lt;/code&gt; role by following the OpenShift Dedicated guide to &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;managing administration roles and users&lt;/a&gt;. Note that the Prometheus examples assume you have admin permissions on the cluster. You'll need to use your GitHub username as the principal here.&lt;/p&gt; &lt;p&gt;Now, you can log in to the console using the &lt;strong&gt;Open Console&lt;/strong&gt; button.&lt;/p&gt; &lt;h2&gt;Install Prometheus and Grafana&lt;/h2&gt; &lt;p&gt;You can install Prometheus on OpenShift Dedicated via the OperatorHub. Just navigate to &lt;strong&gt;Operators -&gt; OperatorHub&lt;/strong&gt;, filter for Prometheus, click &lt;strong&gt;Install&lt;/strong&gt;, and accept the defaults. You can validate that it's installed in the &lt;strong&gt;Installed Operators&lt;/strong&gt; list. Once that's done, do the same for Grafana.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You might have command-line muscle memory and prefer to use &lt;code&gt;kubectl&lt;/code&gt; with a Kubernetes cluster. If you want to take this route, switch to a terminal and copy the login command from the OpenShift console user menu to set up your Kubernetes context.&lt;/p&gt; &lt;h2&gt;Set up Prometheus&lt;/h2&gt; &lt;p&gt;To get Prometheus working with OpenShift Streams for Apache Kafka, use the examples in the Prometheus documentation to create an &lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;additional scrape config&lt;/a&gt;. You will need to make a couple of modifications to your configuration.&lt;/p&gt; &lt;h3&gt;Create an additional config for Prometheus&lt;/h3&gt; &lt;p&gt;First, create the additional config file for Prometheus. To do this, create a file called &lt;code&gt;prometheus-additional.yaml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- job_name: "kafka-federate" static_configs: - targets: ["api.openshift.com"] scheme: "https" metrics_path: "/api/kafkas_mgmt/v1/kafkas/&lt;Your Kafka ID&gt;/metrics/federate" oauth2: client_id: "&lt;Your Service Account Client ID&gt;" client_secret: "Your Service Account Client Secret" token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The angle brackets (&lt;code&gt;&lt;&gt;&lt;/code&gt;) in the listing indicate details you'll need to gather from your own OpenShift Streams for Apache Kafka environment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can see your Kafka ID by clicking on your &lt;strong&gt;Kafka Instance&lt;/strong&gt; in the OpenShift Streams for Apache Kafka console.&lt;/li&gt; &lt;li&gt;Follow OpenShift Streams for Apache Kafka &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;getting started guide&lt;/a&gt; to create a Kafka instance and service account for each instance. As described in the guide, copy and paste the client ID and client secret into &lt;code&gt;prometheus-additional.yaml&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Create a Kubernetes secret&lt;/h3&gt; &lt;p&gt;Now, you need to create a Kubernetes secret that contains this config file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f - -n &lt;namespace&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Apply your changes&lt;/h3&gt; &lt;p&gt;Finally, apply &lt;code&gt;prometheus.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role-binding.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role.yaml&lt;/code&gt;, and &lt;code&gt;prometheus-service-account.yaml&lt;/code&gt; using &lt;code&gt;kubectl apply -f &lt;filename&gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Set up Grafana&lt;/h2&gt; &lt;p&gt;To get Grafana working, use the &lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt; example code from the Grafana project's GitHub repository. Remove the &lt;code&gt;hostname&lt;/code&gt; field from the file, as OpenShift Dedicated will assign the hostname automatically.&lt;/p&gt; &lt;p&gt;Now, find the URL for Grafana from the OpenShift console &lt;strong&gt;Routes&lt;/strong&gt; section, and open Grafana. The login details for Grafana are in the Grafana custom resource.&lt;/p&gt; &lt;p&gt;Next, connect Grafana to Prometheus by navigating to &lt;strong&gt;Settings -&gt; Data Sources&lt;/strong&gt;. Click &lt;strong&gt;Add data source&lt;/strong&gt;, then click &lt;strong&gt;Prometheus&lt;/strong&gt;. At that point, all you need to do is enter &lt;code&gt;http://prometheus-operated:9090&lt;/code&gt;, the URL of the service, then click &lt;strong&gt;Save &amp; Test&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You should now find metrics for your Kafka cluster in Grafana. Figure 2 shows a Grafana dashboard that displays most of the metrics available with OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig2.jpg?itok=dF75OCFB" width="600" height="516" alt="A Grafana dashboard for Red Hat OpenShift Streams for Apache Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A sample Grafana dashboard for OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The JSON that defines this dashboard is &lt;a href="https://github.com/pmuir/rhosak-grafana-dashboard"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I've shown you how to use Prometheus and Grafana to observe an OpenShift Streams for Apache Kafka instance. Prometheus is a very widely adopted format for monitoring and can be used with almost all observability services and products.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" title="How to connect Prometheus to OpenShift Streams for Apache Kafka"&gt;How to connect Prometheus to OpenShift Streams for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pete Muir</dc:creator><dc:date>2021-12-17T07:00:00Z</dc:date></entry><entry><title type="html">Using a JAAS realm in Elytron</title><link rel="alternate" href="https://wildfly-security.github.io/wildfly-elytron/blog/jaas-realm/" /><author><name>Diana Krepinska Vilkolakova</name></author><id>https://wildfly-security.github.io/wildfly-elytron/blog/jaas-realm/</id><updated>2021-12-17T00:00:00Z</updated><dc:creator>Diana Krepinska Vilkolakova</dc:creator></entry><entry><title type="html">The Road Towards a Public API (part 2)</title><link rel="alternate" href="https://blog.kie.org/2021/12/the-road-towards-a-public-api-part-2.html" /><author><name>Edoardo Vacchi</name></author><id>https://blog.kie.org/2021/12/the-road-towards-a-public-api-part-2.html</id><updated>2021-12-16T10:41:54Z</updated><content type="html">In my I described the principles guiding the design of . As I promised last time, in this blog post I would like to give an overview of new API capabilities that this new design would enable. One downside of having an API that is tightly-coupled with the implementation of the engines is that it is harder to evolve, and it is more difficult to support more than API at the same time. The new API is "", in that each functionality is provided by a component that we called a service. For example a service for evaluating a PMML may be simply: interface PredictionService { DataContext evaluate(Id identifier, DataContext ctx); } Each business asset is pointed to using an identifier. The identifier is constructed as a path, which makes it trivially serializable as a string, easy to share and self-descriptive: one identifier contains most of the relevant meta-data about an asset. For example, to refer to a prediction in PMML: /predictions/my.prediction.id Moreover, the path structure makes it natural to refer to nested items in the same asset. For example, to refer to a task in a specific process, for a given process instance: /processes/my.process.id/instances/my.instance.id/tasks/my.task.id In the following, we will explore future possibilities that the design of the new API enables. Currently NONE of the following features are implemented, nor do we have a timeline for delivery. However, we do plan to explore these capabilities in the future. I hope that, by giving you this sneak peek, you will find these design choices convincing, too! DATACONTEXT One of our first encounters in the API is DataContext. A simple interface that denotes an object that can be transformed into another type. we explored how DataContext may denote both data types such as records, as well of generic key-value pairs (MapDataContext). The DataContext basically comes with only two constraints * the first we already mentioned: DataContext should be convertible into another data context * second, the object it denotes should be serializable (into JSON) These "strong" guarantees are also what enables some of the further extensions that we describe in the following. MULTIPLE SERVICE IMPLEMENTATIONS The service API allows for multiple different interaction models to be provided for the same feature. For instance, a classic model would be a synchronous API: interface PredictionService { DataContext evaluate(Id identifier, DataContext ctx); } But nothing prevents engines to opt-in to provide an asynchronous API; for instance: interface AsyncPredictionService { CompletableFuture&lt;DataContext&gt; evaluate(Id identifier, DataContext ctx); } or even, message-based; imagine: messageBus.send( new ProcessStart( app.get(Processes.class).get($id)), MapDataContext.of(Map.of(...))); messageBus.subscribe( "/processes/$id", // ... subscribe and wait for ProcessStarted event ... ) To the point where a generic interface may be provided too. Each service, may implement an interface that all implementation would respect. For instance: interface AsyncGenericService { CompletableFuture&lt;DataContext&gt; evaluate(Id id, Context ctx); } @Inject AsyncGenericService svc; var futureCtx = svc.evaluate(app.get(Processes.class).get($id), MapDataContext.of(...)); In fact, because the identifier is self-descriptive, and the prefix always denotes, by construction, the "top-level" type of the resource that it points to: /processes/... /predictions/... /decisions/... /rule-units/... …it would be possible to define a "gateway" service, responding to the generic interface above, that would then re-route the actual evaluation to the corresponding service; say: /processes/... -&gt; ProcessService /predictions/... -&gt; PredictionService /decisions/... -&gt; DecisionService /rule-units/... -&gt; RuleUnitService STRUCTURED IDENTIFIERS FOR BEHAVIOR BINDING In fact, structured, self-descriptive identifiers open a whole lot of interesting use cases. If anything, being able to univocally, consistently address any component in the platform, allows for binding behavior to components in a simple way. For instance, we may want to listen for the events that the DMN engine produces; then we may declare a listening class, and bind it to the path: // all processes //(currently equivalent to extending ProcessListener) @Kogito("/processes") class MyEventListener { void onStart() { … } } Now suppose that you want to listen to the behavior of a specific process id @Kogito("/processes/my.process.id") class MyEventListener { void onStart() { … } } But we saw that paths may refer to sub-components, too. For instance, tasks. Suppose that you want to define the implementation of a service task. Then you may bind behavior to it as such: // all processes //(currently equivalent to extending ProcessListener) @Kogito("/processes/my.process.id/task/my.task.id") class MyTaskHandler { @Inject ProcessInstanceId piid; SomeResult doSomething(SomeInput in); } LOCAL VS REMOTE ID: DISTRIBUTED EXECUTION So far we only described "paths", i.e. "local" identifiers. A local identifier always starts with a leading slash ‘/’ and its prefix indicates the type of resources (e.g. process, DMN, PMML, rules, etc.) The reason why it is useful for the ID to be a path, is that a path can be "mounted" on a host:port pair, yielding… a URI! For instance, imagine a fictional kogito:// scheme, then we may denote: kogito://my-app@my.remote.host:54321/processes/my.process.id to denote a Kogito application called my-app, reachable at my.remote.host on port 54321. Now wouldn’t it be fancy if you were able to invoke my.process.id using the same service interface ? @Inject AsyncGenericService svc; var localId = app.get(Processes.class).get("my.process.id"); var remoteId = RemoteId.of("my-app@my.remote.host:54321", localId); var futureCtx = svc.evaluate( remoteId, MapDataContext.of(...)); the local application may decode the host part, and direct the request to it. The target, may resolve the request locally, by decoding the "local" part; or it may even act as gateway that re-routes the request to another different distributed service. The identifier is trivially serializable into a string, and the DataContext is by definition serializable as well! MESSAGES: DESIGNING A SERVICE INTERFACE In order to allow for such future extensions, interfaces should be kept simple and small. I would advocate for single-purpose, single-verb methods. Same verbs may apply to different semantics, depending on the structure of the identifier. For instance: ABORT [kogito://$host:$port]/processesABORT all processesABORT /processes/$idABORT the process with ID $idABORT /processes/$id/instances/$instance_idABORT the instance $instance_id of process $id EVAL /decisions/$id PAYLOAD: {  “json-of-data”: …  }EVAL /decisions/$id PAYLOAD: {  “json-of-data”: …  } The benefit of one such design is that it is easy to translate these verb/id/payload triples into messages to send over the wire. For instance, For instance, a simple translation scheme could be adopted to naturally map each message onto a cloud event { "specversion" : "1.0", "type" : "org.kie.kogito.process.eval", "source" : "/processes/my.process.id", "id" : "A234-1234-1234", // request id "time" : "2018-04-05T17:31:00Z", // timestamp "datacontenttype" : "text/json", "data" : { // data context "var1" : "value1", "var2" : "value2" } } MAPPING ONTO HTTP VERBS But a small API surface is easier to map onto other API vocabularies. For instance, it may result very easy to map such a set of commands onto HTTP verbs. This design has emerged from the implementation of the REST endpoints that are already available in Kogito today: POSTEvaluate/StartDELETEAbortPATCHUpdateetc…  These simple mappings may even allow to provide a "generic" style of API that all Kogito services may accept. This kind of generic REST API, may make it simpler for generic, multi-purpose clients to be developed. In other words, it would enable a style of REST API interaction that is more reminiscent of the KIE Server in v7, but without its pitfalls (for instance, without the specialized marshalling format) # starts process POST http://$host:$port/processes/$id BODY { "var1" : "value1", "var2" : "value2" } CONCLUSIONS This concludes our whirlwind tour in all the possible futures of Kogito APIs. As for plans, and ongoing work, we are currently working on delivering a , and the stateful rule API will follow. Next, we will work on the new listener interfaces and the binding mechanism that we have described here. There is quite a bit of work to do, still, but things are looking bright! If you are looking forward to getting your hands on all of this, stay tuned! The post appeared first on .</content><dc:creator>Edoardo Vacchi</dc:creator></entry></feed>
